# Production HLX Brain Training Guide
**100M Parameter Model with Quality Gates & Monitoring**

Complete guide for training a production-ready 100M HLX Brain with all lessons learned from the Opus training failure baked in.

---

## Quick Start

```bash
# Terminal 1: Start Phase 1 training (English mastery)
cd /home/matt/hlx/hlxl_brain
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
python3 train_100m_production.py --phase 1 --epochs 200

# Terminal 2: Start watchdog monitoring
python3 training_watchdog.py --check-interval 60
```

---

## System Overview

### Components Created

1. **validate_checkpoint_quality.py**
   - Detects parroting (10M failure mode)
   - Detects character collapse (250M failure mode)
   - Detects scrambled output (100M failure mode)
   - Tests HLX transformation capability
   - Exit codes: 0=pass, 1=fail, 2=warn

2. **train_100m_production.py**
   - 3-phase training curriculum
   - Corpus validation on load
   - Architecture verification
   - Quality gates at epochs 1, 5, 10, 20, 50, 100, 150, 200
   - Loss curve monitoring
   - Automatic abort on failures
   - Watchdog file updates

3. **training_watchdog.py**
   - Monitors training_status.json
   - Alerts on stalls
   - Alerts on loss divergence
   - Provides ETA updates
   - 60-second check interval

---

## 450-Epoch Training Curriculum

### Phase 1: English Mastery (200 epochs)
**Purpose:** Establish strong English language understanding

```bash
python3 train_100m_production.py \
  --phase 1 \
  --epochs 200 \
  --corpus corpus_all_variants.md \
  --batch-size 4 \
  --seq-length 256 \
  --lr 1e-4
```

**Expected time:** ~15-20 hours
**Checkpoints:**
- `checkpoints/phase1_epoch10.pt`
- `checkpoints/phase1_epoch20.pt`
- ...
- `checkpoints/phase1_BEST_epoch*.pt`
- `checkpoints/phase1_FINAL_epoch200.pt`

**Quality checks:** Epochs 1, 5, 10, 20, 50, 100, 150, 200

---

### Phase 2: HLX Family Mastery (200 epochs)
**Purpose:** Master all 4 HLX formats (HLXL, LC-R, LC-T, LC-B)

```bash
python3 train_100m_production.py \
  --phase 2 \
  --epochs 200 \
  --corpus corpus_all_variants.md \
  --resume-from checkpoints/phase1_FINAL_epoch200.pt \
  --batch-size 4 \
  --seq-length 256 \
  --lr 1e-4
```

**Expected time:** ~15-20 hours
**Checkpoints:** Same pattern as Phase 1

**Quality checks:** Epochs 1, 5, 10, 20, 50, 100, 150, 200

---

### Phase 3: Translation Training (50 epochs)
**Purpose:** Connect English â†” HLX translation capability

```bash
python3 train_100m_production.py \
  --phase 3 \
  --epochs 50 \
  --corpus corpus_all_variants.md \
  --resume-from checkpoints/phase2_FINAL_epoch200.pt \
  --batch-size 4 \
  --seq-length 256 \
  --lr 5e-5
```

**Expected time:** ~4-5 hours
**Checkpoints:** Same pattern as Phase 1

**Quality checks:** Epochs 1, 5, 10, 20, 50

---

## Quality Gates

### What They Check

**Parroting Detection:**
- Model just echoing input back
- Example: "Search for documents" â†’ "Search for documents..."
- Caught by comparing input/output similarity

**Character Collapse:**
- Repetitive character generation
- Example: "Search for documents" â†’ "::::::::pppppp..."
- Caught by regex pattern matching

**Scrambled Output:**
- Random glyphs and nonsense
- Example: "Search for documents" â†’ "000:{@rorm\"s\"ðŸœ1..."
- Caught by symbol-to-letter ratio analysis

**HLX Transformation:**
- Checks if ANY HLX keywords appear in output
- Example: "Search" should produce "SEARCH", "ðŸœŠ", etc.

**Output Diversity:**
- Ensures model doesn't produce identical outputs
- Tests same prompt 3 times with different temperatures

### When They Run

Automatically at these epochs:
- **Epoch 1:** Critical early check
- **Epoch 5:** Verify learning started
- **Epoch 10, 20, 50:** Periodic validation
- **Epoch 100, 150, 200:** Late-stage validation

### What Happens on Failure

1. Quality check runs automatically
2. If check fails: Training **ABORTS immediately**
3. Watchdog status set to `"quality_failed"`
4. Validation results saved to JSON
5. You get detailed failure report

**No wasted compute on broken models.**

---

## Abort Conditions

Training will abort automatically if:

1. **Loss diverged:** `train_loss > 15.0`
2. **Loss collapsed too early:** `train_loss < 0.001` at epoch < 10
3. **Loss increasing:** 5 consecutive epochs of increasing loss
4. **Quality gate failed:** Any quality check fails
5. **Stalled training:** No progress for 5+ minutes (detected by watchdog)

---

## Monitoring

### Real-Time Status

**Watchdog output:**
```
================================================================================
TRAINING WATCHDOG - 2025-12-18 14:30:00
================================================================================
Status: TRAINING
Progress: 47/200 epochs (23.5%)
Train Loss: 2.3421
Val Loss: 2.4156
ETA: 12.3h
Last update: 15s ago
================================================================================
```

### Manual Checks

```bash
# Check training status file
cat training_status.json

# Check latest checkpoint
ls -lh checkpoints/ | tail -5

# Monitor loss progression
tail training_log.txt
```

---

## File Structure

```
hlxl_brain/
â”œâ”€â”€ train_100m_production.py          # Main training script
â”œâ”€â”€ validate_checkpoint_quality.py    # Quality validation
â”œâ”€â”€ training_watchdog.py               # Monitoring script
â”œâ”€â”€ corpus_all_variants.md             # Training corpus
â”œâ”€â”€ checkpoints/                       # Model checkpoints
â”‚   â”œâ”€â”€ phase1_epoch10.pt
â”‚   â”œâ”€â”€ phase1_BEST_epoch47.pt
â”‚   â”œâ”€â”€ phase1_FINAL_epoch200.pt
â”‚   â”œâ”€â”€ phase2_epoch10.pt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ training_status.json               # Real-time status
â”œâ”€â”€ validation_phase1_epoch1.json      # Quality check results
â””â”€â”€ OPUS_TRAINING_FAILURE_REPORT.md    # Lessons learned
```

---

## Memory Optimization (8GB GPU)

The scripts include these optimizations for RTX 5060 8GB:

```python
--batch-size 4              # Small batches
--seq-length 256            # Reduced sequence length
--use-fp16                  # Mixed precision (FP16)
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
```

**Expected VRAM usage:** ~6-7 GB

If OOM still occurs:
- Reduce `--batch-size` to 2
- Reduce `--seq-length` to 128
- Check nothing else is using GPU: `nvidia-smi`

---

## Testing a Checkpoint

After any phase completes, test the model:

```bash
python3 validate_checkpoint_quality.py \
  --checkpoint checkpoints/phase1_FINAL_epoch200.pt \
  --output validation_phase1_final.json
```

**Exit codes:**
- `0`: PASS - model is good
- `1`: FAIL - model has critical issues
- `2`: WARN - model has minor issues

**Output:**
- Detailed JSON report
- Test results for all failure modes
- Generated example outputs

---

## Full Training Workflow

### Day 1-2: Phase 1 (English)
```bash
# Start training
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
python3 train_100m_production.py --phase 1 --epochs 200

# Start watchdog (in another terminal)
python3 training_watchdog.py

# Wait ~15-20 hours
```

### Day 2-3: Phase 2 (HLX)
```bash
# After Phase 1 completes, test it
python3 validate_checkpoint_quality.py \
  --checkpoint checkpoints/phase1_FINAL_epoch200.pt

# Start Phase 2
python3 train_100m_production.py \
  --phase 2 \
  --epochs 200 \
  --resume-from checkpoints/phase1_FINAL_epoch200.pt

# Wait ~15-20 hours
```

### Day 3-4: Phase 3 (Translation)
```bash
# After Phase 2 completes, test it
python3 validate_checkpoint_quality.py \
  --checkpoint checkpoints/phase2_FINAL_epoch200.pt

# Start Phase 3
python3 train_100m_production.py \
  --phase 3 \
  --epochs 50 \
  --resume-from checkpoints/phase2_FINAL_epoch200.pt

# Wait ~4-5 hours
```

### Final Validation
```bash
# Test final model
python3 validate_checkpoint_quality.py \
  --checkpoint checkpoints/phase3_FINAL_epoch50.pt \
  --output validation_100m_FINAL.json

# If PASS, you have a production-ready 100M HLX Brain!
```

---

## Expected Results

**After 450 epochs (200+200+50):**

- **Size:** ~400 MB checkpoint file
- **Parameters:** 101,533,813 (verified 100M)
- **Capability:**
  - Strong English understanding
  - All 4 HLX formats mastered
  - Bidirectional English â†” HLX translation
  - Specialized density: ~855k params/token
  - 24% as dense as GPT-3 (despite being 1,750x smaller)

**Performance prediction:**
- Perfect "junior intern" capability
- Should match 7-13B general models on HLX tasks
- 10-20x cheaper than Sonnet for HLX execution

---

## What's Different from Opus Training?

### Opus Training (FAILED)
- âŒ No architecture verification
- âŒ No corpus validation
- âŒ No quality checks
- âŒ No loss monitoring
- âŒ No watchdog monitoring
- âŒ Trained wrong model sizes (10M, 100M, 250M instead of 1B)
- âŒ All 3 models completely broken

**Result:** 10-15 hours and $30-50 wasted

### Production Training (THIS)
- âœ… Architecture verification at startup
- âœ… Corpus validation with sample checks
- âœ… Quality gates at key epochs
- âœ… Loss curve monitoring with abort conditions
- âœ… Watchdog monitoring every 60 seconds
- âœ… Three properly isolated training phases
- âœ… Comprehensive failure detection

**Result:** High confidence in successful training

---

## Troubleshooting

### "Training stalled at epoch X"
**Check:**
1. Watchdog alerts (should show in terminal)
2. GPU utilization: `nvidia-smi`
3. Process still running: `ps aux | grep train_100m`
4. Disk space: `df -h`

**Fix:** If truly stalled, kill and resume from last checkpoint

### "Quality gate failed at epoch 1"
**Meaning:** Training is fundamentally broken (like Opus)

**Action:**
1. Read validation JSON for details
2. Check corpus loading
3. Verify architecture
4. Adjust learning rate
5. **DO NOT continue training** - it's wasted compute

### "Loss diverged"
**Meaning:** Learning rate too high

**Action:**
1. Training will auto-abort
2. Resume with lower learning rate: `--lr 5e-5`

### "Out of memory"
**Action:**
```bash
# Reduce batch size
--batch-size 2

# Or reduce sequence length
--seq-length 128

# Or both
--batch-size 2 --seq-length 128
```

---

## Future: MoE Experiments

**If 100M training succeeds:**

You can use this as a base for Mixture of Experts experiments:
- Train multiple 100M specialists (different HLX aspects)
- Create router network
- Combine into larger MoE architecture
- Potential: 8x 100M = 800M effective capacity
- Still fits in consumer hardware

---

## Next Steps After Training

1. **Validate Final Model:**
   ```bash
   python3 validate_checkpoint_quality.py \
     --checkpoint checkpoints/phase3_FINAL_epoch50.pt
   ```

2. **Test on Real HLX Tasks:**
   - English â†’ HLX translation
   - HLX â†’ English translation
   - HLX format conversion
   - HLX code generation

3. **Compare to Baselines:**
   - Test same prompts on Sonnet/Opus
   - Measure accuracy and cost
   - Document as "personal intern" capability

4. **Deploy:**
   - Integrate with HLX Dev Studio
   - Create API endpoint
   - Measure cost savings vs Claude

5. **Scale Up (if successful):**
   - MoE experiments
   - Larger models (if you get better GPU)
   - HLX 30B vision?

---

## Summary

**What We Built:**
1. Quality validation suite (detects all known failure modes)
2. Production training script (3-phase curriculum, quality gates)
3. Watchdog monitoring (real-time alerts)

**What We Learned:**
- Always validate checkpoint quality at epoch 1
- Always verify architecture before training
- Always validate corpus loading
- Never trust Opus blindly
- Quality gates prevent wasted compute

**Ready to Train:**
All systems built, tested, and ready for 450-epoch production training.

**Estimated Time:** 40-50 hours total
**Estimated Cost:** Minimal (local training on consumer GPU)
**Confidence Level:** HIGH (all lessons learned baked in)

---

Let's train this bad boy.
