# HLXL Brain Expansion Curriculum
**100 Epoch Training Plan: From Syntax to Semantics**

## Current State (Day 4 Baseline)
- **Parameters:** 491,635
- **Training:** 5 epochs on 164 LC-R examples
- **Capability:** Pattern completion (syntax only, no semantics)
- **Speed:** 700 tok/s
- **Loss:** 0.21 train, 0.23 val

## Training Philosophy
**"Semantic Grounding Through Structured Examples"**

Instead of making the model bigger, we make it *smarter* by teaching it:
1. What operations **mean** (not just their shape)
2. How to **reason** about sequences
3. **Natural language** mappings to LC-R
4. **Perfect HLX syntax** generation

---

## Phase 1: Semantic Grounding (Epochs 6-30, 25 epochs)
**Goal:** Teach the model what operations *mean*, not just their syntax

### Training Focus:
- **Action verbs with context**
  - `navigate` â†’ movement through spaces
  - `search` â†’ finding patterns in data
  - `filter` â†’ selecting subsets
  - `transform` â†’ changing representations
  - `aggregate` â†’ combining many into one
  - `split` â†’ dividing one into many
  - `validate` â†’ checking correctness
  - `optimize` â†’ improving performance

- **Data operations**
  - `read`, `write`, `append`, `delete`
  - `copy`, `move`, `rename`, `backup`
  - `compress`, `decompress`, `encrypt`, `decrypt`

- **Computational operations**
  - `compute`, `calculate`, `evaluate`, `measure`
  - `compile`, `interpret`, `execute`, `debug`
  - `train`, `infer`, `predict`, `classify`

### Corpus Additions (500+ examples):
- Function calls with **semantic comments** in English
- Input/output pairs showing **cause and effect**
- Error cases with **validation messages**

**Example Training Pairs:**
```
English: Navigate to the home directory
LC-R: ğŸœŠ1000ğŸœ0 "navigate"ğŸœ1 âŸhomeğŸœ‚

English: Search for files containing "test"
LC-R: ğŸœŠ1000ğŸœ0 "search"ğŸœ1 âŸfilesğŸœ2 "test"ğŸœ‚

English: Filter data where age is greater than 18
LC-R: ğŸœŠ1000ğŸœ0 "filter"ğŸœ1 âŸdatağŸœ2 ğŸœŠ1000ğŸœ0 "gt"ğŸœ1 âŸageğŸœ2 18ğŸœ‚ğŸœ‚

English: Transform text to uppercase
LC-R: ğŸœŠ1000ğŸœ0 "transform"ğŸœ1 âŸtextğŸœ2 âŸuppercaseğŸœ‚
```

### Success Metrics:
- Model generates **correct operation** given English description
- Can **complete partial LC-R** with semantically appropriate args
- Validation loss < 0.15

---

## Phase 2: Domain Knowledge (Epochs 31-55, 25 epochs)
**Goal:** Teach common patterns, idioms, and domain-specific operations

### Training Focus:
- **File system operations**
  - Paths: `/home/user/docs`, `./relative`, `../parent`
  - Wildcards: `*.txt`, `**/*.py`, `{a,b,c}.json`
  - Permissions: `chmod 755`, `chown user:group`

- **Data structures**
  - Lists: `[1, 2, 3]`, ranges, slices
  - Maps: `{key: value}`, lookups, updates
  - Sets: `{a, b, c}`, unions, intersections
  - Trees: hierarchies, traversals

- **Control flow**
  - Conditionals: `if`, `else`, `match`
  - Loops: `for`, `while`, `iterate`
  - Pipelines: `input | process | output`

- **Common patterns**
  - Map-reduce
  - Filter-transform-aggregate
  - Load-process-save
  - Query-validate-return

### Corpus Additions (600+ examples):
- **Realistic workflows** (multi-step operations)
- **Nested structures** (functions calling functions)
- **Error handling** (try-catch, validate-or-default)

**Example Training Sequences:**
```
English: Load CSV, filter rows, calculate average, save result
LC-R: ğŸœŠ1000ğŸœ0 "pipeline"ğŸœ1
  ğŸœŠ1000ğŸœ0 "load"ğŸœ1 "data.csv"ğŸœ‚ğŸœ2
  ğŸœŠ1000ğŸœ0 "filter"ğŸœ1 âŸrowsğŸœ2 ğŸœŠ1000ğŸœ0 "gt"ğŸœ1 âŸscoreğŸœ2 80ğŸœ‚ğŸœ‚ğŸœ‚ğŸœ3
  ğŸœŠ1000ğŸœ0 "aggregate"ğŸœ1 âŸaverageğŸœ2 âŸscoreğŸœ‚ğŸœ4
  ğŸœŠ1000ğŸœ0 "save"ğŸœ1 "results.json"ğŸœ‚ğŸœ‚

English: Find all Python files, count lines, sort by size
LC-R: ğŸœŠ1000ğŸœ0 "pipeline"ğŸœ1
  ğŸœŠ1000ğŸœ0 "find"ğŸœ1 "**/*.py"ğŸœ‚ğŸœ2
  ğŸœŠ1000ğŸœ0 "map"ğŸœ1 ğŸœŠ1000ğŸœ0 "count_lines"ğŸœ‚ğŸœ‚ğŸœ3
  ğŸœŠ1000ğŸœ0 "sort"ğŸœ1 âŸdescğŸœ‚ğŸœ‚
```

### Success Metrics:
- Can generate **multi-step workflows** from English descriptions
- Correctly **nests function calls**
- Validation loss < 0.12

---

## Phase 3: Long-Form Reasoning (Epochs 56-80, 25 epochs)
**Goal:** Handle longer sequences, maintain context, generate complex programs

### Training Focus:
- **Extended sequences** (256-512 tokens)
- **Context maintenance** (remember earlier args)
- **Variable binding** (define once, use many times)
- **Conditional logic** (if-then-else chains)
- **Iteration** (loops with state)

### Training Techniques:
- **Increase sequence length** from 128 â†’ 256 tokens
- **Add positional encodings** for longer context
- **Curriculum learning**: Short â†’ Medium â†’ Long sequences
- **Attention analysis**: Verify model attends to relevant parts

### Corpus Additions (400+ examples):
- **Long programs** (10-20 operations)
- **Stateful operations** (counters, accumulators)
- **Recursive patterns** (tree traversals)

**Example Long Sequence:**
```
English: Build a data processing pipeline that loads multiple files,
         merges them, cleans nulls, validates schema, transforms columns,
         aggregates by group, and exports to three formats

LC-R: ğŸœŠ1000ğŸœ0 "define"ğŸœ1 âŸpipelineğŸœ2
  ğŸœŠ1000ğŸœ0 "sequence"ğŸœ1
    ğŸœŠ1000ğŸœ0 "load_many"ğŸœ1 ["data1.csv", "data2.csv", "data3.csv"]ğŸœ‚ğŸœ2
    ğŸœŠ1000ğŸœ0 "merge"ğŸœ1 âŸonğŸœ2 "id"ğŸœ‚ğŸœ3
    ğŸœŠ1000ğŸœ0 "filter"ğŸœ1 ğŸœŠ1000ğŸœ0 "not_null"ğŸœ1 âŸvalueğŸœ‚ğŸœ‚ğŸœ4
    ğŸœŠ1000ğŸœ0 "validate"ğŸœ1 âŸschemağŸœ2 "data.schema.json"ğŸœ‚ğŸœ5
    ğŸœŠ1000ğŸœ0 "transform"ğŸœ1 âŸnormalizeğŸœ2 ["col1", "col2"]ğŸœ‚ğŸœ6
    ğŸœŠ1000ğŸœ0 "group_by"ğŸœ1 "category"ğŸœ2 ğŸœŠ1000ğŸœ0 "sum"ğŸœ1 âŸrevenueğŸœ‚ğŸœ‚ğŸœ7
    ğŸœŠ1000ğŸœ0 "export"ğŸœ1 ["results.csv", "results.json", "results.parquet"]ğŸœ‚ğŸœ‚ğŸœ‚
```

### Success Metrics:
- Can generate **coherent long sequences** (>200 tokens)
- Maintains **context** across operations
- Validation loss < 0.10

---

## Phase 4: Perfect HLX + Quality English (Epochs 81-105, 25 epochs)
**Goal:** Master HLX family syntax + generate natural, idiomatic English

### Training Focus:

#### HLX Family Mastery:
- **LC-R (Latent Collapse Runic)** - Core format
- **LC-B (Latent Collapse Binary)** - Wire format
- **HLXL (Helix Language)** - High-level syntax
- **All contract types** (14-22, 1000-1002)
- **Perfect glyph balance** (ğŸœŠ starts, ğŸœ‚ ends, ğŸœ separates)
- **Proper nesting** (recursive structures)
- **Type safety** (contracts wrapping values correctly)

#### Natural Language Generation:
- **Idiomatic English** (not robotic translations)
- **Context-aware phrasing** (adapt to domain)
- **Concise descriptions** (remove verbosity)
- **Technical accuracy** (proper terminology)

### Corpus Additions (500+ examples):
- **Bidirectional pairs**: English â†” LC-R
- **Multiple phrasings**: Same LC-R, different English
- **Style variations**: Formal, casual, technical
- **Error corrections**: Bad syntax â†’ Good syntax

**Example Pairs (English variations for same LC-R):**
```
LC-R: ğŸœŠ1000ğŸœ0 "search"ğŸœ1 âŸdatabaseğŸœ2 "keyword"ğŸœ‚

English (formal): Execute a search operation on the database for the keyword
English (casual): Search the database for "keyword"
English (terse): db.search("keyword")
English (verbose): Perform a comprehensive search query against the database system to locate entries matching the specified keyword
```

**Example Perfect HLX Syntax:**
```
HLXL: search(database, "keyword")
LC-R: ğŸœŠ1000ğŸœ0 "search"ğŸœ1 âŸdatabaseğŸœ2 "keyword"ğŸœ‚
LC-B: [0x08][0x0F][0x00][0x06]search[0x0F][0x01][0x14]database[0x0F][0x02][0x10][0x07]keyword[0x0E]

All equivalent, perfect syntax, deterministic conversion
```

### Advanced Training Examples:
```
# Complex nested structure with perfect balance
LC-R: ğŸœŠ1000ğŸœ0 "pipeline"ğŸœ1
  ğŸœŠ14ğŸœ0
    ğŸœŠ1000ğŸœ0 "load"ğŸœ1 "input.json"ğŸœ‚ğŸœ1
    ğŸœŠ1000ğŸœ0 "transform"ğŸœ1 âŸnormalizeğŸœ‚ğŸœ2
    ğŸœŠ1000ğŸœ0 "filter"ğŸœ1 ğŸœŠ1000ğŸœ0 "valid"ğŸœ‚ğŸœ‚ğŸœ3
    ğŸœŠ1000ğŸœ0 "save"ğŸœ1 "output.json"ğŸœ‚ğŸœ‚ğŸœ‚

English: Run a data pipeline that loads input.json, normalizes the data,
         filters for valid entries, and saves to output.json
```

### Success Metrics:
- **Zero syntax errors** in generated LC-R
- **Perfect glyph balance** (validated by parser)
- **Natural English** (human evaluation)
- **All HLX formats** generated correctly
- Validation loss < 0.08
- **Final target: < 0.05 val loss**

---

## Training Schedule

### Hardware Target:
- **CPU-only training** (for portability)
- **Batch size:** 4-8 (memory efficient)
- **Learning rate schedule:** Cosine annealing with warm restarts
- **Checkpointing:** Every 5 epochs + best model
- **Early stopping:** Patience of 10 epochs

### Time Estimates (CPU):
- **Phase 1:** ~6-8 hours (25 epochs, 500 examples)
- **Phase 2:** ~8-10 hours (25 epochs, 600 examples)
- **Phase 3:** ~10-12 hours (25 epochs, 400 longer examples)
- **Phase 4:** ~8-10 hours (25 epochs, 500 examples)
- **Total:** ~32-40 hours of training

### Monitoring:
- **Loss curves** (train/val) plotted each phase
- **Perplexity** tracking
- **Sample generation** every 5 epochs (qualitative check)
- **Validation on held-out test set** (never seen during training)

---

## Corpus Statistics (Target)

### Current:
- **164 examples** (LC-R syntax only)
- **115 vocabulary tokens**
- **~20,000 training tokens**

### After Expansion:
- **~2,000 examples** (164 baseline + 1,836 new)
  - Phase 1: +500 (semantic grounding)
  - Phase 2: +600 (domain knowledge)
  - Phase 3: +400 (long sequences)
  - Phase 4: +336 (perfect syntax + English)
- **~250,000 training tokens** (12.5x increase)
- **Vocabulary:** Still 115 (no expansion needed)

---

## Evaluation Framework

### Quantitative Metrics:
1. **Validation loss** (target: < 0.05)
2. **Perplexity** (lower is better)
3. **BLEU score** (LC-R generation vs reference)
4. **Exact match accuracy** (glyph balance, syntax)

### Qualitative Metrics:
1. **Semantic correctness** (does it do what English says?)
2. **Natural language quality** (human readability)
3. **Edge case handling** (rare patterns)
4. **Error recovery** (graceful degradation)

### Test Suite:
- **100 held-out examples** (never trained on)
- **Adversarial cases** (tricky syntax)
- **Cross-validation** (5-fold)
- **Ablation studies** (which phase mattered most?)

---

## Deliverables

### Checkpoints:
- `checkpoint_epoch30_phase1.pt` (semantic grounding complete)
- `checkpoint_epoch55_phase2.pt` (domain knowledge complete)
- `checkpoint_epoch80_phase3.pt` (long-form reasoning complete)
- `checkpoint_epoch105_phase4.pt` (final model)
- `best_model.pt` (lowest validation loss)

### Documentation:
- `TRAINING_LOG.md` (detailed progress notes)
- `ABLATION_STUDY.md` (which training helped most?)
- `GENERATION_SAMPLES.md` (examples from each phase)

### Benchmarks:
- `benchmarks/phase1_results.json`
- `benchmarks/phase2_results.json`
- `benchmarks/phase3_results.json`
- `benchmarks/phase4_results.json`
- `benchmarks/final_comparison.json` (before/after)

---

## Expected Outcomes

### By Epoch 30 (Phase 1):
- âœ… Generates **semantically correct** LC-R for English prompts
- âœ… Understands **30+ operation types**
- âœ… Can **complete partial sequences** intelligently

### By Epoch 55 (Phase 2):
- âœ… Handles **multi-step workflows**
- âœ… Correctly **nests function calls**
- âœ… Knows **file paths, data structures, control flow**

### By Epoch 80 (Phase 3):
- âœ… Generates **long, coherent programs** (200+ tokens)
- âœ… Maintains **context** across operations
- âœ… Can **reason about state**

### By Epoch 105 (Phase 4):
- âœ… **Zero syntax errors** in LC-R
- âœ… **Perfect HLX family syntax**
- âœ… **Natural, idiomatic English**
- âœ… Ready for **production deployment**

---

## Risk Mitigation

### Overfitting Prevention:
- **Data augmentation** (paraphrase, reorder, permute)
- **Dropout** (0.1 during training)
- **Weight decay** (L2 regularization)
- **Validation monitoring** (stop if val loss increases)

### Catastrophic Forgetting:
- **Replay buffer** (keep 20% old examples in each phase)
- **Gradual curriculum** (don't drop old tasks)
- **Multi-task learning** (train on all phases simultaneously in final 5 epochs)

### Computational Constraints:
- **Gradient checkpointing** (reduce memory usage)
- **Mixed precision training** (FP16 where safe)
- **Batch accumulation** (simulate larger batches)

---

## Success Criteria

**The HLXL Brain is ready for production when:**

1. âœ… Validation loss < 0.05
2. âœ… 99%+ syntax correctness on held-out test set
3. âœ… Human evaluators rate English quality as "natural" (4/5+)
4. âœ… Can generate 500+ token sequences without errors
5. âœ… Inference speed still > 500 tok/s (minimal degradation)
6. âœ… Model size < 3 MB (still tiny!)

---

## Next Steps

1. **Build expanded corpus** (phases 1-4)
2. **Modify training script** for continued training
3. **Set up monitoring dashboard** (live loss curves)
4. **Run Phase 1** (25 epochs)
5. **Evaluate & iterate**
6. **Repeat for Phases 2-4**

**Estimated completion:** 2-3 days of continuous training
**Expected model quality:** Production-ready HLX semantic engine
